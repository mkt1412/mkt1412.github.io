<!DOCTYPE HTML>
<html lang="en"><head><!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-CZ8RH708DQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-CZ8RH708DQ');
  </script><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chao Tang</title>
  
  <meta name="author" content="Chao Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;400;600&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css">
  
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">  
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:80%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chao Tang</name>
                <!-- [&#128264 Tzu-Ray Hs&uuml] -->
              </p>
              <p>Hi, Iâ€™m a Robotics Ph.D. student in Electronic and Electrical Engineering at <a href="https://www.sustech.edu.cn/en/">Southern University of Science and Technology (SUSTech)</a>, 
                advised by Chair Professor <a href="https://faculty.sustech.edu.cn/?tagid=zhangh33&iscss=1&snapid=1&orderby=date&go=1&lang=en">Hong Zhang</a>. I'm also a visiting student in the School of Computing at the National University of Singapore (NUS), advised by Chair Professor <a href="https://www.comp.nus.edu.sg/cs/people/dyhsu/">David Hsu</a>. 
                My research interests involve robotic manipulation 
                and grasping, mobile manipulation, human-robot interaction, and semantic reasoning for robots. The ultimate goal is to develop autonomous agents that can perceive, reason, and interact with the physical world with the 
                same level of Intelligence as humans.
                <!-- In particular, my work investigates: 
                1) <b>provably near-optimal algorithms</b> for online learning and combinatorial optimization; and
                2) <b>robot learning and coordination algorithms</b> that enable effective and efficient online decision-making in dynamic and adversarial environments. -->
                <!-- for complex tasks in dynamic and adversarial environments where robots efficiently learn near-optimal actions on the fly . -->
              </p>
              <p>Previously, I received my M.S. in <a href="https://www.ece.gatech.edu/">Electrical & Computer Engineering</a> from <a href="https://www.gatech.edu/">Georgia Tech</a> in 2020 (advised by Prof. <a href="https://ece.gatech.edu/directory/patricio-antonio-vela">Patricio Vela</a>) 
                and my B.Eng. in Automation from <a href="https://en.njtech.edu.cn/">Nanjing Tech University</a> in 2018.
              </p>
              <p>Feel free to reach out if you're interested in my research or would like to collaborate!
              </p>
              <p style="text-align:center">
                <a href="mailto:12131042@mail.sustech.edu.cn"><b>email</b></a> &nbsp/&nbsp
                <a href="data/CV.pdf"><b>CV</b></a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=hXGhWsUAAAAJ&hl=en"><b>google scholar</b></a> &nbsp/&nbsp
                <a href="https://twitter.com/njtangchao96"><b>Twitter</b></a>
                <!-- <a href="https://github.com/zirui-xu/">Github</a> -->
                <!-- <a href="https://www.researchgate.net/profile/Zirui-Xu-6"><b>researchgate</b></a> &nbsp/&nbsp -->
                <!-- <a href="https://www.linkedin.com/in/ziruixu/"><b>linkedin</b></a> -->
                
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile1.jpeg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li>
                  <span style="color:gray">09/27/2024:</span> If you are attending IROS 2024, make sure to stop by our oral presentation titled "<a href="https://sites.google.com/view/csg-os">Commonsense Scene Graph-based Target Localization for Object Search</a>"! (Session: Semantic Scene Understanding III, Time: 11:15-11:30, October 18, 2024)
                <li>
                <li>
                  <span style="color:gray">09/16/2024:</span> Check out our ICRA 2025 submission on task-oriented grasping learning from huma demonstration videos, titled "<a href="https://sites.google.com/view/rtagrasp/home">RTAGrasp: Learning Task-Oriented Grasping from Human Videos via Retrieval, Transfer, and Alignment</a>". Code implementation will be publicly available soon.
                <li>
                <li>
                  <span style="color:gray">06/30/2024:</span> Thrilled to announce that our work titled "<a href="https://sites.google.com/view/csg-os">Commonsense Scene Graph-based Target Localization for Object Search</a>" has bee accpted to IROS 2024 as oral presentation. Stay tuned for the code implementation.
                <li>
                  <span style="color:gray">05/20/2024:</span> Enjoying a fantastic time in Japan at ICRA 2024! It's great to catch up with old friends and make new ones. See you all next time!
                </li>
                <li>
                  <span style="color:gray">04/17/2024:</span> Presenting our latest work "<a href="https://sites.google.com/view/foundationgrasp">FoundationGrasp: Generalizable Task-Oriented Grasping with Foundation Models</a>", which is an extension from our previous RA-L work <a href="https://sites.google.com/view/graspgpt">GraspGPT</a>.
                </li>
                <li>
                  <span style="color:gray">04/06/2024:</span> We are presenting our works on LLM-based grasping (Robotics with Large Language Models) and multi-view rearrangement (Perception for Grasping and Manipulation III) at <em>IEEE ICRA 2024</em> on 16th May. Make sure to stop by. See you in Yokohama!
                </li>
                <li>
                  <span style="color:gray">04/02/2024:</span> I have redirected my hosting institution to the National University of Singapore and will be a visiting Ph.D. student at AdaComp Lab for 12 months, supervised by Prof. David Hsu.
                </li>
                <li>
                  <span style="color:gray">03/20/2024:</span> Check out our latest work, titled "<a href="https://arxiv.org/abs/2404.00343">Commonsense Scene Graph-based Target Localization for Object Search</a>", under review by IROS 2024.
                </li>
                <li>
                  <span style="color:gray">01/29/2024:</span> First accpted paper of 2024! Our work on object rearrangement, titled "<a href="https://arxiv.org/pdf/2309.08994.pdf">Efficient Object Rearrangement via Multi-view Fusion</a>", has been accpted by <em>IEEE ICRA 2024</em>!
                </li>
                <li>
                  <span style="color:gray">11/30/2023:</span> I have successfully passed the PhD dissertation proposal titled "Vision Language Scene Understanding for Task-Oriented Grasping". Slides are publicy available <a href="https://docs.google.com/presentation/d/1AdZAzgyKLFfOn1NCEwPbR-QUY_7lwQtJ/edit?usp=sharing&ouid=100337134101880148367&rtpof=true&sd=true">here</a>.
                </li>
              </ul>
              <heading>Selected Publications</heading>
              <ol>
                <li>
                  <papertitle>FoundationGrasp: Generalizable Task-Oriented Grasping with Foundation Models</papertitle>
                  <br>
                  <b>Chao Tang</b>,
                  Dehao Huang, 
                  Wenlong Dong,
                  Ruinian Xu,
                  Hong Zhang
                  <br>
                  <em>Under Review</em>, 2024.
                  <br>
                  <a href="https://arxiv.org/pdf/2404.10399.pdf">preprint</a> / <a href="https://sites.google.com/view/foundationgrasp">website</a> / <a href="https://github.com/mkt1412/GraspGPT_public">code</a> / <a href="https://www.youtube.com/watch?v=0AEwXl9i77o&feature=youtu.be">video</a> 
                </li>
                <br>
                <li>
                  <papertitle>GraspGPT: Leveraging Semantic Knowledge from A Large Language Model for Task-Oriented Grasping</papertitle>
                  <br>
                  <b>Chao Tang</b>,
                  Dehao Huang, 
                  Wenqi Ge,
                  Weiyu Liu,
                  Hong Zhang
                  <br>
                  <em>IEEE Robotics and Automation Letters</em> (RA-L), 2023.
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10265134">paper</a> / <a href="https://sites.google.com/view/graspgpt">website</a> / <a href="https://github.com/mkt1412/GraspGPT_public">code</a> / <a href="https://www.youtube.com/watch?v=qq0DMdHRw1E">video</a> 
                </li>
                <br>
                <li>
                  <papertitle>Task-Oriented Grasp Prediction with Visual-Language Inputs</papertitle>
                  <br>
                  <b>Chao Tang</b>,
                  Dehao Huang, 
                  Lingxiao Meng,
                  Weiyu Liu,
                  Hong Zhang
                  <br>
                  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems </em>(IROS), 2023.
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10342268">paper</a> / <a href="https://arxiv.org/pdf/2307.13204.pdf">arxiv</a> / <a href="https://www.youtube.com/watch?v=e1wfYQPeAXU">video</a> 
                </li>
                <br>
                <li>
                  <papertitle>Relationship Oriented Semantic Scene Understanding for Daily Manipulation Tasks</papertitle>
                  <br>
                  <b>Chao Tang</b>,
                  Jingwen Yu, 
                  Weinan Chen,
                  Bingyi Xia,
                  Hong Zhang
                  <br>
                  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems </em> (IROS), 2022.
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9981960">paper</a> / <a href="https://drive.google.com/file/d/1wL3XmJt_-VYIq7cO1lMo0I09TyBVkuGw/view?usp=sharing">video</a> /  <a href="https://docs.google.com/presentation/d/10UzGuVYANGRN6nSMplZWPKoEPs4NUu-XantlLYUSQsg/edit?usp=sharing">slides</a>
                </li>
                <br>
                <li>
                  <papertitle>An affordance keypoint detection network for robot manipulation</papertitle>
                  <br>
                  Ruinian Xu,
                  Fujen Chu,
                  <b>Chao Tang</b>,
                  Patricio Vela
                  <br>
                  <em>IEEE Robotics and Automation Letters</em>  (RA-L), 2021.
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9364360">paper</a> / <a href="https://github.com/ivalab/AffKpNet">code</a>
                </li>
                <br>
                <li>
                  <papertitle>Using synthetic data and deep networks to recognize primitive shapes for object grasping</papertitle>
                  <br>
                  Yunzhi Lin<sup>*</sup>,
                  <b>Chao Tang</b><sup>*</sup>,
                  Fujen Chu,
                  Patricio Vela (*-equal contribution)
                  <br>
                  <em>IEEE International Conference on Robotics and Automation</em>  (ICRA), 2020.
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9197256">paper</a> / <a href="https://sites.google.com/view/primitive-shape-grasping/home">website</a> /<a href="https://github.com/ivalab/grasp_primitiveShape">code</a> / <a href="https://www.youtube.com/watch?v=AZuLpEzQMYQ">video</a>
                </li>
              </ol>

              <heading>Teaching</heading>
              <ul>
                <li>
                  Graduate Teaching Assistant: EE 5346 - Autonomous Robot Navigation, Spring 2023.
                </li>
                <li>
                  Graduate Teaching Assistant: EE 101 - Electronic and Information Technology for Metaverse, Fall 2023.
                </li>
                <li>
                  Graduate Teaching Assistant: EE 5058 - Introduction to Information Technology, Spring 2024.
                </li>
              </ul>
              <heading>Activities and Services</heading>
              <ul>
                <li>
                  IEEE Transactions on Automation Science and Engineering (T-ASE) Reviewer
                </li>
                <li>
                  IEEE Transactions on Industrial Informatics (TII) Reviewer
                </li>
                <li>
                  IEEE Robotics and Automation Letters (RAL) Reviewer
                </li>
                <li>
                  IEEE ICRA Reviewer
                </li>
                <li>
                  IEEE/RSJ IROS Reviewer
                </li>
                <li>
                  IEEE ICRA 2021 Organizing Committee & Outstanding Volunteer
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>
          <tr>
            <td style="padding:2.5%">
              <br>
              <!-- <p style="font-size:small">Last updated: Apr 26, 2021</p>
              <p style="text-align:left;font-size:small"> &copy 2021 Zirui Xu</p>
              <p style="text-align:left;font-size:small">
                <a style="font-size:small" href="https://jonbarron.info">This guy makes a cool website.</a>
              </p> -->
              <p style="text-align:center;font-size:small;color:gray">last updated: Dec. 2023 | template from <a style="font-size:small" href="https://jonbarron.info">here</a></p>
            </td>  
          </tr>        
        </tbody></table>
        
      </td>
    </tr>
  </table>
</body>

</html>
