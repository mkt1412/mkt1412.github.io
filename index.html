<!DOCTYPE HTML>
<html lang="en"><head><!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-CZ8RH708DQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-CZ8RH708DQ');
  </script><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chao Tang</title>
  
  <meta name="author" content="Chao Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;400;600&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css">
  
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">  
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:80%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chao Tang</name>
                <!-- [&#128264 Tzu-Ray Hs&uuml] -->
              </p>
              <p>Hi, Iâ€™m a Robotics Ph.D. student in Electronic and Electrical Engineering at <a href="https://www.sustech.edu.cn/en/">Southern University of Science and Technology (SUSTech)</a>, 
                advised by Chair Professor <a href="https://faculty.sustech.edu.cn/?tagid=zhangh33&iscss=1&snapid=1&orderby=date&go=1&lang=en">Hong Zhang</a>. My research interests involve robotic manipulation 
                and grasping, mobile manipulation, human-robot interaction, and semantic reasoning for robots. The ultimate goal is to develop autonomous agents that can perceive, reason, and interact with the physical world with the 
                same level of Intelligence as humans.
                <!-- In particular, my work investigates: 
                1) <b>provably near-optimal algorithms</b> for online learning and combinatorial optimization; and
                2) <b>robot learning and coordination algorithms</b> that enable effective and efficient online decision-making in dynamic and adversarial environments. -->
                <!-- for complex tasks in dynamic and adversarial environments where robots efficiently learn near-optimal actions on the fly . -->
              </p>
              <p>Previously, I received my M.S. in <a href="https://www.ece.gatech.edu/">Electrical & Computer Engineering</a> from <a href="https://www.gatech.edu/">Georgia Tech</a> in 2020 (advised by Prof. <a href="https://ece.gatech.edu/directory/patricio-antonio-vela">Patricio Vela</a>) 
                and my B.Eng. in Automation from <a href="https://en.njtech.edu.cn/">Nanjing Tech University</a> in 2018.
              </p>
              <p style="text-align:center">
                <a href="mailto:12131042@mail.sustech.edu.cn"><b>email</b></a> &nbsp/&nbsp
                <a href="data/CV.pdf"><b>CV</b></a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=hXGhWsUAAAAJ&hl=en"><b>google scholar</b></a> &nbsp/&nbsp
                <a href="https://twitter.com/njtangchao96"><b>Twitter</b></a>
                <!-- <a href="https://github.com/zirui-xu/">Github</a> -->
                <!-- <a href="https://www.researchgate.net/profile/Zirui-Xu-6"><b>researchgate</b></a> &nbsp/&nbsp -->
                <!-- <a href="https://www.linkedin.com/in/ziruixu/"><b>linkedin</b></a> -->
                
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile1.jpeg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li>
                  <span style="color:gray">04/17/2024:</span> Presenting our latest work "<a href="https://sites.google.com/view/foundationgrasp">FoundationGrasp: Generalizable Task-Oriented Grasping with Foundation Models</a>", which is an extension from our previous RA-L work <a href="https://sites.google.com/view/graspgpt">GraspGPT</a>.
                </li>
                <li>
                  <span style="color:gray">04/06/2024:</span> We are presenting our works on LLM-based grasping (Robotics with Large Language Models) and multi-view rearrangement (Perception for Grasping and Manipulation III) at <em>IEEE ICRA 2024</em> on 16th May. Make sure to stop by. See you in Yokohama!
                </li>
                <li>
                  <span style="color:gray">04/02/2024:</span> I have redirected my CSC hosting institution to the National University of Singapore and will be a visiting Ph.D. student at AdaComp Lab for 12 months, supervised by Prof. David Hsu.
                </li>
                <li>
                  <span style="color:gray">03/20/2024:</span> Check out our latest work, titled "<a href="https://arxiv.org/abs/2404.00343">Commonsense Scene Graph-based Target Localization for Object Search</a>", under review by IROS 2024.
                </li>
                <li>
                  <span style="color:gray">01/29/2024:</span> First accpted paper of 2024! Our work on object rearrangement, titled "<a href="https://arxiv.org/pdf/2309.08994.pdf">Efficient Object Rearrangement via Multi-view Fusion</a>", has been accpted by <em>IEEE ICRA 2024</em>!
                </li>
                <li>
                  <span style="color:gray">11/30/2023:</span> I have successfully passed the PhD dissertation proposal titled "Vision Language Scene Understanding for Task-Oriented Grasping". Slides are publicy available <a href="https://docs.google.com/presentation/d/1AdZAzgyKLFfOn1NCEwPbR-QUY_7lwQtJ/edit?usp=sharing&ouid=100337134101880148367&rtpof=true&sd=true">here</a>.
                </li>
                <li>
                  <span style="color:gray">11/14/2023:</span> I will be attending Autonomous Robotic Technology Seminar (ARTS) held by Chinese Association of Automation (CAA) at HKUST GZ from 11.25 to 11.26. 
                </li>
                <li>
                  <span style="color:gray">09/19/2023:</span> Thrilled to announce that <a href="https://sites.google.com/view/graspgpt">GraspGPT</a> has been officially accepted by <em>IEEE Robotics and Automation Letters</em>. Great thanks to all the collaborators!
                </li>
                <li>
                  <span style="color:gray">07/26/2023:</span> Presenting our latest work titled "<a href="https://sites.google.com/view/graspgpt">GraspGPT: Leveraging Semantic Knowledge from A Large Language Model for Task-Oriented Grasping</a>". Arxiv preprint, project website, video, and code implementation are publicly available.
                </li>
              </ul>
              <heading>Selected Publications</heading>
              <ol>
                <li>
                  <papertitle>FoundationGrasp: Generalizable Task-Oriented Grasping with Foundation Models</papertitle>
                  <br>
                  <b>Chao Tang</b>,
                  Dehao Huang, 
                  Wenlong Dong,
                  Ruinian Xu,
                  Hong Zhang
                  <br>
                  <em>Under Review</em>, 2024.
                  <br>
                  <a href="https://arxiv.org/pdf/2404.10399.pdf">pre-print</a> / <a href="https://sites.google.com/view/foundationgrasp">website</a> / code (coming soon) / <a href="https://www.youtube.com/watch?v=0AEwXl9i77o&feature=youtu.be">video</a> 
                </li>
                <br>
                <li>
                  <papertitle>GraspGPT: Leveraging Semantic Knowledge from A Large Language Model for Task-Oriented Grasping</papertitle>
                  <br>
                  <b>Chao Tang</b>,
                  Dehao Huang, 
                  Wenqi Ge,
                  Weiyu Liu,
                  Hong Zhang
                  <br>
                  <em>IEEE Robotics and Automation Letters</em> (RA-L), 2023.
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10265134">paper</a> / <a href="https://sites.google.com/view/graspgpt">website</a> / <a href="https://github.com/mkt1412/GraspGPT_public">code</a> / <a href="https://www.youtube.com/watch?v=qq0DMdHRw1E">video</a> 
                </li>
                <br>
                <li>
                  <papertitle>Task-Oriented Grasp Prediction with Visual-Language Inputs</papertitle>
                  <br>
                  <b>Chao Tang</b>,
                  Dehao Huang, 
                  Lingxiao Meng,
                  Weiyu Liu,
                  Hong Zhang
                  <br>
                  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems </em>(IROS), 2023.
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10342268">paper</a> / <a href="https://arxiv.org/pdf/2307.13204.pdf">arxiv</a> / <a href="https://www.youtube.com/watch?v=e1wfYQPeAXU">video</a> 
                </li>
                <br>
                <li>
                  <papertitle>Relationship Oriented Semantic Scene Understanding for Daily Manipulation Tasks</papertitle>
                  <br>
                  <b>Chao Tang</b>,
                  Jingwen Yu, 
                  Weinan Chen,
                  Bingyi Xia,
                  Hong Zhang
                  <br>
                  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems </em> (IROS), 2022.
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9981960">paper</a> / <a href="https://drive.google.com/file/d/1wL3XmJt_-VYIq7cO1lMo0I09TyBVkuGw/view?usp=sharing">video</a> /  <a href="https://docs.google.com/presentation/d/10UzGuVYANGRN6nSMplZWPKoEPs4NUu-XantlLYUSQsg/edit?usp=sharing">slides</a>
                </li>
                <br>
                <li>
                  <papertitle>An affordance keypoint detection network for robot manipulation</papertitle>
                  <br>
                  Ruinian Xu,
                  Fujen Chu,
                  <b>Chao Tang</b>,
                  Patricio Vela
                  <br>
                  <em>IEEE Robotics and Automation Letters</em>  (RA-L), 2021.
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9364360">paper</a> / <a href="https://github.com/ivalab/AffKpNet">code</a>
                </li>
                <br>
                <li>
                  <papertitle>Using synthetic data and deep networks to recognize primitive shapes for object grasping</papertitle>
                  <br>
                  Yunzhi Lin<sup>*</sup>,
                  <b>Chao Tang</b><sup>*</sup>,
                  Fujen Chu,
                  Patricio Vela (*-equal contribution)
                  <br>
                  <em>IEEE International Conference on Robotics and Automation</em>  (ICRA), 2020.
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9197256">paper</a> / <a href="https://sites.google.com/view/primitive-shape-grasping/home">website</a> /<a href="https://github.com/ivalab/grasp_primitiveShape">code</a> / <a href="https://www.youtube.com/watch?v=AZuLpEzQMYQ">video</a>
                </li>
              </ol>

              <heading>Teaching</heading>
              <ul>
                <li>
                  Graduate Teaching Assistant: EE 5346 - Autonomous Robot Navigation, Spring 2023.
                </li>
                <li>
                  Graduate Teaching Assistant: EE 101 - Electronic and Information Technology for Metaverse, Fall 2023.
                </li>
                <li>
                  Graduate Teaching Assistant: EE 5058 - Introduction to Information Technology, Spring 2024.
                </li>
              </ul>
              <heading>Activities and Services</heading>
              <ul>
                <li>
                  IEEE Transactions on Industrial Informatics (TII) Reviewer
                </li>
                <li>
                  IEEE Robotics and Automation Letters (RAL) Reviewer
                </li>
                <li>
                  IEEE ICRA Reviewer
                </li>
                <li>
                  IEEE/RSJ IROS Reviewer
                </li>
                <li>
                  IEEE ICRA 2021 Organizing Committee & Outstanding Volunteer
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>
          <tr>
            <td style="padding:2.5%">
              <br>
              <!-- <p style="font-size:small">Last updated: Apr 26, 2021</p>
              <p style="text-align:left;font-size:small"> &copy 2021 Zirui Xu</p>
              <p style="text-align:left;font-size:small">
                <a style="font-size:small" href="https://jonbarron.info">This guy makes a cool website.</a>
              </p> -->
              <p style="text-align:center;font-size:small;color:gray">last updated: Dec. 2023 | template from <a style="font-size:small" href="https://jonbarron.info">here</a></p>
            </td>  
          </tr>        
        </tbody></table>
        
      </td>
    </tr>
  </table>
</body>

</html>
